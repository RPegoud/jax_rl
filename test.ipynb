{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "from jax import random, lax, jit, vmap, pmap\n",
    "from tqdm.auto import tqdm\n",
    "from functools import partial\n",
    "\n",
    "from policies import BasePolicy\n",
    "from agents import Q_learning\n",
    "from envs import GridWorld\n",
    "from policies import EpsilonGreedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "SEED = 2\n",
    "GRID_DIM = jnp.array([8, 8])\n",
    "INITIAL_STATE = jnp.array([8, 8])\n",
    "GOAL_STATE = jnp.array([0, 0])\n",
    "GRID_SIZE = jnp.array([8, 8])\n",
    "N_STATES = jnp.prod(GRID_DIM)\n",
    "N_ACTIONS = 4\n",
    "DISCOUNT = 0.9\n",
    "LEARNING_RATE = 0.1\n",
    "\n",
    "key = random.PRNGKey(SEED)\n",
    "\n",
    "env = GridWorld(INITIAL_STATE, GOAL_STATE, GRID_SIZE)\n",
    "policy = EpsilonGreedy(0.1)\n",
    "agent = Q_learning(\n",
    "    key,\n",
    "    N_STATES,\n",
    "    N_ACTIONS,\n",
    "    DISCOUNT,\n",
    "    LEARNING_RATE,\n",
    "    policy,\n",
    ")\n",
    "\n",
    "q_values = jnp.zeros([GRID_SIZE[0], GRID_SIZE[1], N_ACTIONS], dtype=jnp.float32)\n",
    "movements = jnp.array([[0, 1], [1, 0], [0, -1], [-1, 0]])\n",
    "\n",
    "env_state, obs = env.reset(key) \n",
    "done = False\n",
    "\n",
    "steps = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "for _ in tqdm(range(400)):\n",
    "    done = False\n",
    "    n_steps = 0\n",
    "\n",
    "    while not done:\n",
    "        state, _ = env_state\n",
    "        action, key = policy(key, N_ACTIONS, state, q_values)\n",
    "        movement = movements[action]\n",
    "        env_state, obs, reward, done = env.step(env_state, movement)\n",
    "\n",
    "        q_values = agent.update(state, action, reward, done, obs, q_values)\n",
    "        n_steps+=1\n",
    "    steps.append(n_steps)\n",
    "\n",
    "\n",
    "px.imshow(pd.DataFrame(jnp.max(q_values, axis=2)).round(2), title=\"Maximal Q-value for each state\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_step = jit(vmap(env.step, \n",
    "                  in_axes=((0, 0), 0), # ((env_state), action)\n",
    "                  out_axes=((0, 0), 0, 0, 0), # ((env_state), obs, reward, done)\n",
    "                  axis_name=\"batch_axis\"\n",
    "                  ))\n",
    "\n",
    "v_reset = jit(vmap(env.reset,\n",
    "                   out_axes=((0,0), 0), # ((env_state), obs)\n",
    "                   axis_name=\"batch_axis\"\n",
    "                   ))\n",
    "\n",
    "v_policy = jit(vmap(policy.call, \n",
    "                in_axes=(0, None, 1, -1), # (keys, n_actions, state, q_values) \n",
    "                axis_name=\"batch_axis\"\n",
    "                ),\n",
    "                static_argnums=(1,)\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array(2, dtype=int32), Array([2425776485,  230565590], dtype=uint32))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# @partial(jit, static_argnums=(1))\n",
    "# def policy(key, n_actions, state, q_values):\n",
    "#     def _random_action_fn(subkey):\n",
    "#         return random.choice(subkey, jnp.arange(n_actions))\n",
    "\n",
    "#     def _greedy_action_fn(subkey):\n",
    "#         \"\"\"\n",
    "#         Selects the greedy action with random tie-break\n",
    "#         \"\"\"\n",
    "#         q = q_values[state[0], state[1]]\n",
    "#         q_max = jnp.max(q, axis=-1)\n",
    "#         q_max_mask = jnp.equal(q, q_max)\n",
    "#         p = jnp.divide(q_max_mask, q_max_mask.sum())\n",
    "#         choice = random.choice(subkey, jnp.arange(n_actions), p=p)\n",
    "#         return jnp.int32(choice)\n",
    "\n",
    "#     explore = random.uniform(key) < 0.1\n",
    "#     key, subkey = random.split(key)\n",
    "#     action = lax.cond(\n",
    "#         explore,\n",
    "#         _random_action_fn,\n",
    "#         _greedy_action_fn,\n",
    "#         operand=subkey,\n",
    "#     )\n",
    "\n",
    "#     return action, subkey\n",
    "\n",
    "# v_policy = vmap(policy, \n",
    "#                 in_axes=(0, None, 0, -1), # (keys, n_actions, state, q_values) \n",
    "#                 axis_name=\"batch_axis\"\n",
    "#                 )\n",
    "                \n",
    "# N_ENV = 10\n",
    "# key = random.PRNGKey(SEED)\n",
    "# keys = random.split(key, N_ENV)\n",
    "# env_state, obs = v_reset(keys)\n",
    "# q_values = jnp.zeros([GRID_SIZE[0], GRID_SIZE[1], N_ACTIONS, N_ENV], dtype=jnp.float32)\n",
    "\n",
    "# action, keys = v_policy(keys, N_ACTIONS, states, q_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "vmap got inconsistent sizes for array axes to be mapped:\n  * most axes (2 of them) had size 10, e.g. axis 0 of argument key of type uint32[10,2];\n  * one axis had size 2: axis 1 of argument state of type int32[10,2]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ryanp\\OneDrive\\Bureau\\Taff\\grind_arc\\src\\jax_rl\\test.ipynb Cell 7\u001b[0m line \u001b[0;36m8\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ryanp/OneDrive/Bureau/Taff/grind_arc/src/jax_rl/test.ipynb#W6sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m states, keys \u001b[39m=\u001b[39m env_state\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ryanp/OneDrive/Bureau/Taff/grind_arc/src/jax_rl/test.ipynb#W6sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m q_values \u001b[39m=\u001b[39m jnp\u001b[39m.\u001b[39mzeros([GRID_SIZE[\u001b[39m0\u001b[39m], GRID_SIZE[\u001b[39m1\u001b[39m], N_ACTIONS, N_ENV], dtype\u001b[39m=\u001b[39mjnp\u001b[39m.\u001b[39mfloat32)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ryanp/OneDrive/Bureau/Taff/grind_arc/src/jax_rl/test.ipynb#W6sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m action, keys \u001b[39m=\u001b[39m v_policy(keys, N_ACTIONS, states, q_values)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ryanp/OneDrive/Bureau/Taff/grind_arc/src/jax_rl/test.ipynb#W6sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m state, key \u001b[39m=\u001b[39m env_state\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ryanp/OneDrive/Bureau/Taff/grind_arc/src/jax_rl/test.ipynb#W6sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# env_state, obs, reward, done = v_step(env_state, action)\u001b[39;00m\n",
      "    \u001b[1;31m[... skipping hidden 14 frame]\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\ryanp\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\grind-arc--jAx_NNy-py3.10\\lib\\site-packages\\jax\\_src\\api.py:1340\u001b[0m, in \u001b[0;36m_mapped_axis_size\u001b[1;34m(fn, tree, vals, dims, name)\u001b[0m\n\u001b[0;32m   1338\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1339\u001b[0m     msg\u001b[39m.\u001b[39mappend(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m  * some axes (\u001b[39m\u001b[39m{\u001b[39;00mct\u001b[39m}\u001b[39;00m\u001b[39m of them) had size \u001b[39m\u001b[39m{\u001b[39;00msz\u001b[39m}\u001b[39;00m\u001b[39m, e.g. axis \u001b[39m\u001b[39m{\u001b[39;00max\u001b[39m}\u001b[39;00m\u001b[39m of \u001b[39m\u001b[39m{\u001b[39;00mex\u001b[39m}\u001b[39;00m\u001b[39m;\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 1340\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(msg)[:\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m])\n",
      "\u001b[1;31mValueError\u001b[0m: vmap got inconsistent sizes for array axes to be mapped:\n  * most axes (2 of them) had size 10, e.g. axis 0 of argument key of type uint32[10,2];\n  * one axis had size 2: axis 1 of argument state of type int32[10,2]"
     ]
    }
   ],
   "source": [
    "N_ENV = 10\n",
    "key = random.PRNGKey(SEED)\n",
    "keys = random.split(key, N_ENV)\n",
    "env_state, obs = v_reset(keys)\n",
    "states, keys = env_state\n",
    "q_values = jnp.zeros([GRID_SIZE[0], GRID_SIZE[1], N_ACTIONS, N_ENV], dtype=jnp.float32)\n",
    "\n",
    "action, keys = v_policy(keys, N_ACTIONS, states, q_values)\n",
    "\n",
    "state, key = env_state\n",
    "# env_state, obs, reward, done = v_step(env_state, action)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "grind-arc--jAx_NNy-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
