{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ryanp\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\jax-rl-KPtyfD6I-py3.10\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import optax\n",
    "import haiku as hk\n",
    "\n",
    "from jax import random, lax, jit, vmap, pmap\n",
    "from functools import partial\n",
    "from jax_tqdm import loop_tqdm\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../../../\")\n",
    "\n",
    "from src import CartPole, DQN, EpsilonGreedy, UniformReplayBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 2\n",
    "DISCOUNT = 0.9\n",
    "LEARNING_RATE = 0.1\n",
    "N_ACTIONS = 2\n",
    "NEURONS_PER_LAYER = [128, 256, N_ACTIONS]\n",
    "BUFFER_SIZE = 512\n",
    "BATCH_SIZE = 32\n",
    "TIME_STEPS = 100_000\n",
    "STATE_SHAPE = 4\n",
    "LEARNING_RATE = 1e-2\n",
    "EPSILON = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'actions': (512,), 'dones': (512,), 'next_states': (512, 4), 'rewards': (512,), 'states': (512, 4)}\n"
     ]
    }
   ],
   "source": [
    "buffer_state = {\n",
    "    \"states\": jnp.empty((BUFFER_SIZE, STATE_SHAPE), dtype=jnp.float32),\n",
    "    \"actions\": jnp.empty((BUFFER_SIZE,), dtype=jnp.int32),\n",
    "    \"rewards\": jnp.empty((BUFFER_SIZE,), dtype=jnp.int32),\n",
    "    \"next_states\": jnp.empty((BUFFER_SIZE, STATE_SHAPE), dtype=jnp.float32),\n",
    "    \"dones\": jnp.empty((BUFFER_SIZE,), dtype=jnp.bool_),\n",
    "}\n",
    "print(jax.tree_map(lambda x: x.shape, buffer_state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_key = random.PRNGKey(SEED)\n",
    "\n",
    "env = CartPole()\n",
    "policy = EpsilonGreedy(0.1)\n",
    "\n",
    "\n",
    "@hk.transform\n",
    "def model(x):\n",
    "    mlp = hk.nets.MLP(output_sizes=NEURONS_PER_LAYER)\n",
    "    return mlp(x)\n",
    "\n",
    "\n",
    "replay_buffer = UniformReplayBuffer(BUFFER_SIZE, BATCH_SIZE)\n",
    "\n",
    "model_params = model.init(action_key, jnp.zeros((STATE_SHAPE,)))\n",
    "target_net_params = model.init(action_key, jnp.zeros((STATE_SHAPE,)))\n",
    "optimizer = optax.adam(learning_rate=LEARNING_RATE)\n",
    "optimizer_state = optimizer.init(model_params)\n",
    "\n",
    "agent = DQN(DISCOUNT, LEARNING_RATE, model, EPSILON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mlp/~/linear_0': {'b': (128,), 'w': (4, 128)},\n",
       " 'mlp/~/linear_1': {'b': (256,), 'w': (128, 256)},\n",
       " 'mlp/~/linear_2': {'b': (2,), 'w': (256, 2)}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.tree_map(lambda x: x.shape, model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(ScaleByAdamState(count=(), mu={'mlp/~/linear_0': {'b': (128,), 'w': (4, 128)}, 'mlp/~/linear_1': {'b': (256,), 'w': (128, 256)}, 'mlp/~/linear_2': {'b': (2,), 'w': (256, 2)}}, nu={'mlp/~/linear_0': {'b': (128,), 'w': (4, 128)}, 'mlp/~/linear_1': {'b': (256,), 'w': (128, 256)}, 'mlp/~/linear_2': {'b': (2,), 'w': (256, 2)}}),\n",
       " EmptyState())"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.tree_map(lambda x: x.shape, optimizer_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "action_key = random.PRNGKey(0)\n",
    "exp = (random.normal(action_key, (4,)), 1, 1, random.normal(action_key, (4,)), False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "init_key = random.split(action_key)[0]\n",
    "model_params = model.init(init_key, random.normal(init_key, (4,)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "env_state, obs = env.reset(action_key)\n",
    "env.step(env_state, jnp.array([1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "action_key, subkey = random.split(action_key)\n",
    "experiences = replay_buffer.sample(action_key, buffer_state, 10)\n",
    "agent.update(model_params, target_net_params, optimizer, optimizer_state, experiences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **_Rollout_**\n",
    "\n",
    "1. Init replay buffer\n",
    "2. for t steps:\n",
    "   1. action = agent.act\n",
    "   2. add experience to replay buffer\n",
    "   3. sample batch from replay buffer\n",
    "   4. agent.update\n",
    "      1. every N steps, update target network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESTEPS = 100\n",
    "RANDOM_SEED = 0\n",
    "TARGET_NET_UPDATE_FREQ = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running for 100,000 iterations: 100%|██████████| 100000/100000 [00:28<00:00, 3484.82it/s]\n"
     ]
    }
   ],
   "source": [
    "def rollout(\n",
    "    timesteps: int,\n",
    "    random_seed: int,\n",
    "    target_net_update_freq: int,\n",
    "    model: hk.Transformed,\n",
    "    optimizer: optax.GradientTransformation,\n",
    "    buffer_state: dict,\n",
    "):\n",
    "    @loop_tqdm(timesteps)\n",
    "    def _fori_body(i: int, val: tuple):\n",
    "        (\n",
    "            model_params,\n",
    "            target_net_params,\n",
    "            optimizer_state,\n",
    "            buffer_state,\n",
    "            action_key,\n",
    "            buffer_key,\n",
    "            env_state,\n",
    "            all_obs,\n",
    "            all_rewards,\n",
    "            all_done,\n",
    "        ) = val\n",
    "\n",
    "        state, _ = env_state\n",
    "        action, action_key = agent.act(action_key, model_params, state)\n",
    "        env_state, obs, reward, done = env.step(env_state, action)\n",
    "        experience = (state, action, reward, obs, done)\n",
    "\n",
    "        buffer_state = replay_buffer.add(buffer_state, experience, i)\n",
    "        current_buffer_size = jnp.min(jnp.array([i, BUFFER_SIZE]))\n",
    "        experiences_batch, buffer_key = replay_buffer.sample(\n",
    "            buffer_key, buffer_state, current_buffer_size\n",
    "        )\n",
    "\n",
    "        model_params, optimizer_state = agent.update(\n",
    "            model_params,\n",
    "            target_net_params,\n",
    "            optimizer,\n",
    "            optimizer_state,\n",
    "            experiences_batch,\n",
    "        )\n",
    "\n",
    "        target_net_params = lax.cond(\n",
    "            i % target_net_update_freq,\n",
    "            lambda _: model_params,\n",
    "            lambda _: target_net_params,\n",
    "            operand=None,\n",
    "        )\n",
    "\n",
    "        all_obs = all_obs.at[i].set(obs)\n",
    "        all_rewards = all_rewards.at[i].set(reward)\n",
    "        all_done = all_done.at[i].set(done)\n",
    "\n",
    "        val = (\n",
    "            model_params,\n",
    "            target_net_params,\n",
    "            optimizer_state,\n",
    "            buffer_state,\n",
    "            action_key,\n",
    "            buffer_key,\n",
    "            env_state,\n",
    "            all_obs,\n",
    "            all_rewards,\n",
    "            all_done,\n",
    "        )\n",
    "\n",
    "        return val\n",
    "\n",
    "    init_key, action_key, buffer_key = vmap(random.PRNGKey)(jnp.arange(3) + random_seed)\n",
    "    env_state, _ = env.reset(init_key)\n",
    "    all_obs = jnp.zeros([timesteps, STATE_SHAPE])\n",
    "    all_rewards = jnp.zeros([timesteps], dtype=jnp.int32)\n",
    "    all_done = jnp.zeros([timesteps], dtype=jnp.bool_)\n",
    "\n",
    "    model_params = model.init(action_key, jnp.zeros((STATE_SHAPE,)))\n",
    "    target_net_params = model.init(action_key, jnp.zeros((STATE_SHAPE,)))\n",
    "    optimizer_state = optimizer.init(model_params)\n",
    "\n",
    "    val_init = (\n",
    "        model_params,\n",
    "        target_net_params,\n",
    "        optimizer_state,\n",
    "        buffer_state,\n",
    "        action_key,\n",
    "        buffer_key,\n",
    "        env_state,\n",
    "        all_obs,\n",
    "        all_rewards,\n",
    "        all_done,\n",
    "    )\n",
    "\n",
    "    val = lax.fori_loop(0, timesteps, _fori_body, val_init)\n",
    "    model_params = val[0]\n",
    "    all_rewards, all_done = val[-2], val[-1]\n",
    "    return model_params, all_rewards, all_done\n",
    "\n",
    "\n",
    "model_params, all_rewards, all_done = rollout(\n",
    "    100_000,\n",
    "    RANDOM_SEED,\n",
    "    TARGET_NET_UPDATE_FREQ,\n",
    "    model,\n",
    "    optimizer,\n",
    "    buffer_state,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([False, False, False, ..., False, False, False], dtype=bool)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "init_key, action_key, buffer_key = vmap(random.PRNGKey)(jnp.arange(3) + 1)\n",
    "env_state, _ = env.reset(init_key)\n",
    "all_obs = jnp.zeros([TIMESTEPS, STATE_SHAPE])\n",
    "all_rewards = jnp.zeros([TIMESTEPS], dtype=jnp.int32)\n",
    "all_done = jnp.zeros([TIMESTEPS], dtype=jnp.bool_)\n",
    "\n",
    "model_params = model.init(action_key, jnp.zeros((STATE_SHAPE,)))\n",
    "target_net_params = model.init(action_key, jnp.zeros((STATE_SHAPE,)))\n",
    "optimizer_state = optimizer.init(model_params)\n",
    "\n",
    "state, _ = env_state\n",
    "action, action_key = agent.act(action_key, model_params, state)\n",
    "env_state, obs, reward, done = env.step(env_state, action)\n",
    "experience = (state, action, reward, obs, done)\n",
    "buffer_state = replay_buffer.add(buffer_state, experience, 1)\n",
    "experiences_batch, buffer_key = replay_buffer.sample(buffer_key, buffer_state, 1)\n",
    "model_params, optimizer_state = agent.update(\n",
    "    model_params,\n",
    "    target_net_params,\n",
    "    optimizer,\n",
    "    optimizer_state,\n",
    "    experiences_batch,\n",
    ")\n",
    "\n",
    "target_net_params = lax.cond(\n",
    "    1 % 10,\n",
    "    lambda _: model_params,\n",
    "    lambda _: target_net_params,\n",
    "    operand=None,\n",
    ")\n",
    "\n",
    "all_obs = all_obs.at[1].set(obs)\n",
    "all_rewards = all_rewards.at[1].set(reward)\n",
    "all_done = all_done.at[1].set(done)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax-rl-KPtyfD6I-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
